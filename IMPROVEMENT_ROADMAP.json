{
  "current_status": {
    "model": "Qwen2.5-VL-3B + LoRA (r=32)",
    "training_progress": "250/500 steps (50%)",
    "phase2a_cer": 0.32,
    "phase2a_wer": 0.68,
    "phase2a_exact_match": 0.24
  },
  "improvement_phases": {
    "Phase_2B_PostProcessing": {
      "target": 0.26,
      "description": "Quick wins through pattern-based post-processing without retraining",
      "techniques": [
        "Odia-specific spell correction (ସଠିକ → ସଠିକ)",
        "Diacritical mark correction (ମୁଁ → ମୁଁ consistency)",
        "Language model reranking with Odia LLM",
        "Confidence-based filtering (low-confidence flag)",
        "Common OCR error patterns (ସ vs ଣ discrimination)",
        "Numeral standardization (modern vs traditional numerals)"
      ],
      "effort": "Medium",
      "time": "1-2 weeks",
      "improvement": "5-8% CER reduction"
    },
    "Phase_2C_ModelEnhancement": {
      "target": 0.2,
      "description": "Improve model capacity and learning without full retraining",
      "techniques": [
        "LoRA rank increase (32→64)",
        "Multi-scale feature extraction",
        "Cross-attention mechanism for complex characters",
        "Residual connections for gradient flow",
        "Mixed precision training improvements"
      ],
      "effort": "High",
      "time": "2-3 weeks",
      "improvement": "6-8% CER reduction"
    },
    "Phase_3_FullRetraining": {
      "target": 0.15,
      "description": "Full training to convergence (500+ steps) with optimized hyperparameters",
      "techniques": [
        "Complete training to 500 steps",
        "Dynamic learning rate scheduling",
        "Curriculum learning (easy → hard samples)",
        "Data augmentation (rotation, scaling, noise)",
        "Weighted loss for character frequency",
        "Early stopping with validation monitoring"
      ],
      "effort": "Medium",
      "time": "3-4 days GPU time",
      "improvement": "5-7% CER reduction"
    },
    "Phase_4_AdvancedOptimization": {
      "target": 0.08,
      "description": "Production-grade optimization techniques",
      "techniques": [
        "Knowledge distillation from larger models",
        "Ensemble voting with diverse approaches",
        "Quantization for deployment (INT8)",
        "Semantic re-ranking with context-aware LLM",
        "Confidence calibration for production",
        "Structured prediction (enforce valid Odia sequences)"
      ],
      "effort": "High",
      "time": "4-6 weeks",
      "improvement": "7-10% CER reduction"
    },
    "Phase_5_Domain_Specialization": {
      "target": 0.05,
      "description": "Adapt to specific Odia document types (books, newspapers, historical)",
      "techniques": [
        "Domain-specific fine-tuning (literature, legal, technical)",
        "Historical script variant handling",
        "Handwritten vs printed document detection",
        "Metadata-aware inference (document type hints)",
        "Multi-task learning (OCR + script identification)"
      ],
      "effort": "High",
      "time": "6-8 weeks",
      "improvement": "3-5% CER reduction"
    }
  },
  "olmocr_bench_adaptation": {
    "test_categories": [
      {
        "name": "Odia Script Basics",
        "description": "Basic Odia character and word recognition",
        "test_samples": 500,
        "focus": [
          "Character confusion (ସ vs ଶ vs ଷ)",
          "Diacriticals (ୁ, ୀ)",
          "Numerals"
        ],
        "evaluation_metric": "Character accuracy"
      },
      {
        "name": "Diacritical Marks",
        "description": "Complex diacritical combinations",
        "test_samples": 300,
        "focus": [
          "Stacked diacritics",
          "Consonant clusters",
          "Vowel signs"
        ],
        "evaluation_metric": "Diacritical accuracy"
      },
      {
        "name": "Literature & Classic Texts",
        "description": "Traditional Odia literary works",
        "test_samples": 200,
        "focus": [
          "Old Odia script variants",
          "Complex sentence structures",
          "Proper nouns"
        ],
        "evaluation_metric": "Semantic preservation"
      },
      {
        "name": "Multi-column Documents",
        "description": "Newspaper and magazine layouts",
        "test_samples": 150,
        "focus": [
          "Reading order preservation",
          "Column detection",
          "Header/footer handling"
        ],
        "evaluation_metric": "Reading order accuracy"
      },
      {
        "name": "Handwritten Text",
        "description": "Handwritten Odia documents",
        "test_samples": 100,
        "focus": [
          "Handwriting variation",
          "Letter connectivity",
          "Background noise"
        ],
        "evaluation_metric": "Handwritten CER"
      },
      {
        "name": "Tables & Structured Data",
        "description": "Tabular data extraction",
        "test_samples": 150,
        "focus": [
          "Cell boundaries",
          "Numerical accuracy",
          "Alignment"
        ],
        "evaluation_metric": "Table cell accuracy"
      },
      {
        "name": "Mathematical & Scientific Content",
        "description": "Equations and formulas with Odia text",
        "test_samples": 100,
        "focus": [
          "Mixed Odia-Math content",
          "Formula recognition",
          "Variable names"
        ],
        "evaluation_metric": "Formula accuracy"
      }
    ],
    "evaluation_metrics": {
      "CER": "Character Error Rate (Levenshtein distance)",
      "WER": "Word Error Rate",
      "Exact_Match": "Percentage of exact matches",
      "Diacritical_Accuracy": "Correct diacritical marks",
      "Reading_Order": "Correct text sequence (for multi-column)",
      "Semantic_Preservation": "Meaning intact after OCR",
      "Confidence_Calibration": "Prediction confidence accuracy"
    },
    "baseline_targets": {
      "current": {
        "CER": 0.32,
        "WER": 0.68,
        "Exact_Match": 0.24
      },
      "phase_2b": {
        "CER": 0.26,
        "WER": 0.6,
        "Exact_Match": 0.32
      },
      "phase_2c": {
        "CER": 0.2,
        "WER": 0.52,
        "Exact_Match": 0.4
      },
      "phase_3": {
        "CER": 0.15,
        "WER": 0.45,
        "Exact_Match": 0.5
      },
      "phase_4": {
        "CER": 0.08,
        "WER": 0.3,
        "Exact_Match": 0.7
      },
      "phase_5": {
        "CER": 0.05,
        "WER": 0.2,
        "Exact_Match": 0.85
      }
    }
  },
  "techniques": {
    "1_TrainingDataQuality": {
      "title": "Training Data Quality Improvements",
      "description": "Better data = better model",
      "actions": [
        {
          "action": "Data Deduplication",
          "details": "Remove duplicate samples (145K → ~130K unique)",
          "effort": "Low",
          "impact": "2-3% CER improvement",
          "code": "hash-based duplicate detection"
        },
        {
          "action": "Hard Negative Mining",
          "details": "Identify samples where model fails most",
          "effort": "Medium",
          "impact": "3-5% CER improvement",
          "code": "confidence-based filtering + active learning"
        },
        {
          "action": "Balanced Sampling",
          "details": "Ensure equal representation of character types",
          "effort": "Low",
          "impact": "1-2% CER improvement",
          "code": "stratified sampling by character groups"
        },
        {
          "action": "Data Augmentation",
          "details": "Synthetic variations: rotation, scaling, noise, blur",
          "effort": "Medium",
          "impact": "4-6% CER improvement",
          "code": "imgaug or Albumentations library"
        }
      ]
    },
    "2_HyperparameterOptimization": {
      "title": "Hyperparameter Tuning",
      "description": "Fine-tune training configuration",
      "actions": [
        {
          "action": "Learning Rate Schedule",
          "details": "Use cosine annealing or warmup-decay",
          "current": "Fixed 2e-4",
          "recommended": "Warmup 1e-4 → 5e-4 → cosine decay",
          "impact": "2-3% CER improvement"
        },
        {
          "action": "Batch Size Optimization",
          "details": "Test batch_size in [2, 4, 8, 16]",
          "current": "4",
          "recommended": "8 (if VRAM allows) or gradient accumulation",
          "impact": "1-2% CER improvement"
        },
        {
          "action": "Gradient Accumulation",
          "details": "Effective batch size without VRAM increase",
          "recommended": "Accumulation steps = 2-4",
          "impact": "1-2% CER improvement"
        },
        {
          "action": "Mixed Precision Training",
          "details": "FP16 with loss scaling",
          "current": "Already using float16",
          "recommended": "Add loss scaling and verify gradients",
          "impact": "0.5-1% CER improvement + faster training"
        }
      ]
    },
    "3_ArchitectureImprovements": {
      "title": "Model Architecture Enhancements",
      "description": "Better model design",
      "actions": [
        {
          "action": "Increase LoRA Rank",
          "details": "r=32 → r=64 for more capacity",
          "effort": "Low",
          "impact": "2-4% CER improvement",
          "trade_off": "28MB → ~56MB adapter"
        },
        {
          "action": "LoRA Alpha Tuning",
          "details": "Increase alpha to improve adaptation",
          "current": "Default (often r)",
          "recommended": "alpha = 2*r (= 128)",
          "impact": "1-2% CER improvement"
        },
        {
          "action": "Add Attention Layers",
          "details": "Fine-tune more attention layers (target_modules)",
          "current": "Likely visual and text attention",
          "recommended": "Add cross-attention modules",
          "impact": "2-3% CER improvement"
        },
        {
          "action": "Layer-wise Transfer",
          "details": "Different learning rates for different layers",
          "effort": "High",
          "impact": "2-3% CER improvement"
        }
      ]
    },
    "4_PostProcessing": {
      "title": "Post-Processing Techniques",
      "description": "Fix errors after generation",
      "actions": [
        {
          "action": "Odia Spell Correction",
          "details": "Dictionary + edit distance for corrections",
          "libraries": [
            "symspellpy",
            "pyspellchecker"
          ],
          "impact": "3-5% CER improvement",
          "effort": "Medium"
        },
        {
          "action": "Language Model Reranking",
          "details": "Use Odia LLM to score and rerank predictions",
          "libraries": [
            "GPT-2 Odia",
            "XLNET Odia"
          ],
          "impact": "4-6% CER improvement",
          "effort": "High"
        },
        {
          "action": "Diacritical Mark Consistency",
          "details": "Enforce consistency for same characters",
          "effort": "Low",
          "impact": "1-2% CER improvement"
        },
        {
          "action": "Context-Based Correction",
          "details": "Use surrounding context to fix errors",
          "effort": "High",
          "impact": "2-4% CER improvement"
        }
      ]
    },
    "5_EnsembleEnhancements": {
      "title": "Ensemble & Voting Strategies",
      "description": "Combine multiple approaches",
      "actions": [
        {
          "action": "Temperature Sampling Ensemble",
          "details": "Generate multiple outputs with T=0.7,0.8,0.9,1.0",
          "effort": "Low",
          "impact": "2-3% CER improvement",
          "trade_off": "4x inference time"
        },
        {
          "action": "Checkpoint Ensemble + Voting",
          "details": "Ensemble multiple checkpoint improvements",
          "current": "Only checkpoint-250",
          "recommended": "Use checkpoints from different training phases",
          "impact": "3-4% CER improvement"
        },
        {
          "action": "Diverse Model Ensemble",
          "details": "Combine different base models (Qwen + Llama + Claude-style)",
          "effort": "High",
          "impact": "5-8% CER improvement"
        }
      ]
    },
    "6_ConfidenceCalibration": {
      "title": "Confidence-Based Quality Estimation",
      "description": "Know when model is uncertain",
      "actions": [
        {
          "action": "Token Confidence Scores",
          "details": "Compute per-token confidence from logits",
          "effort": "Low",
          "impact": "Enables filtering (e.g., use only >90% confidence)",
          "code": "softmax of logits"
        },
        {
          "action": "Sequence Confidence Ranking",
          "details": "Average token confidence as sequence score",
          "effort": "Low",
          "impact": "Rank multiple candidates"
        },
        {
          "action": "Selective Prediction",
          "details": "Reject low-confidence predictions",
          "effort": "Medium",
          "impact": "Better precision (trade-off: recall)"
        }
      ]
    }
  },
  "timeline": {
    "Week_1_Phase2B": {
      "start_cer": 0.32,
      "expected_cer": 0.26,
      "actions": [
        "Spell correction (odia dictionary)",
        "LM reranking (Odia GPT-2)",
        "Ensemble voting improvements"
      ],
      "effort_days": 5
    },
    "Week_2_Phase2C": {
      "start_cer": 0.26,
      "expected_cer": 0.2,
      "actions": [
        "LoRA rank increase (32→64)",
        "Multi-scale feature extraction",
        "Attention mechanism refinement"
      ],
      "effort_days": 7
    },
    "Week_3_Phase3": {
      "start_cer": 0.2,
      "expected_cer": 0.15,
      "actions": [
        "Complete training to 500 steps",
        "Curriculum learning",
        "Data augmentation impact"
      ],
      "effort_days": 3
    },
    "Month_2_Phase4": {
      "start_cer": 0.15,
      "expected_cer": 0.08,
      "actions": [
        "Knowledge distillation",
        "Quantization (INT8)",
        "Advanced ensemble"
      ],
      "effort_days": 20
    },
    "Month_3_Phase5": {
      "start_cer": 0.08,
      "expected_cer": 0.05,
      "actions": [
        "Domain specialization",
        "Handwriting adaptation",
        "Production optimization"
      ],
      "effort_days": 30
    }
  },
  "priority_actions": {
    "week_1": [
      "Run model on olmOCR-Bench adapted for Odia",
      "Implement spell correction (Phase 2B)",
      "Create data augmentation pipeline",
      "Benchmark improvements"
    ],
    "month_1": [
      "Complete Phase 2B: 32% → 26% CER",
      "Implement Phase 2C: 26% → 20% CER",
      "Begin Phase 3: Train to 500 steps"
    ],
    "month_3": [
      "Reach <15% CER (production ready)",
      "Implement Phase 4-5 specializations",
      "Deploy production model"
    ]
  },
  "expected_final_performance": {
    "cer": 0.05,
    "wer": 0.2,
    "exact_match": 0.85,
    "inference_time": "5-8 sec (optimized)",
    "production_ready": true
  }
}