#!/usr/bin/env python3
"""
PHASE 2 PRODUCTION: Beam Search + Ensemble Inference Pipeline
Production-ready implementation for improved Odia OCR predictions

This module provides:
1. Beam search inference with configurable beam width
2. Ensemble voting from multiple checkpoints
3. Temperature-based sampling options
4. Batch processing support
5. Performance monitoring
"""

import torch
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from PIL import Image
from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration
from peft import PeftModel
import warnings

warnings.filterwarnings("ignore")


class OdiaOCRInferenceEngine:
    """Production OCR inference engine with optimization techniques"""
    
    def __init__(
        self,
        model_name: str = "Qwen/Qwen2.5-VL-3B-Instruct",
        checkpoint_dir: str = "./qwen_odia_ocr_improved_v2",
        device: str = "auto",
        precision: torch.dtype = torch.float16,
    ):
        """Initialize inference engine"""
        
        self.model_name = model_name
        self.checkpoint_dir = checkpoint_dir
        self.device = device
        self.precision = precision
        
        print(f"üöÄ Initializing Odia OCR Inference Engine")
        print(f"   Model: {model_name}")
        print(f"   Checkpoint dir: {checkpoint_dir}")
        print(f"   Precision: {precision}")
        
        # Load processor
        self.processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)
        print(f"   ‚úÖ Processor loaded")
        
        # Load base model
        self.base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            model_name,
            torch_dtype=precision,
            trust_remote_code=True,
            device_map=device,
        )
        print(f"   ‚úÖ Base model loaded")
        
        self.base_model.eval()
        torch.cuda.empty_cache()
    
    def load_checkpoint(self, checkpoint_name: str) -> PeftModel:
        """Load LoRA checkpoint"""
        checkpoint_path = Path(self.checkpoint_dir) / checkpoint_name
        
        if not checkpoint_path.exists():
            raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")
        
        model = PeftModel.from_pretrained(
            self.base_model,
            str(checkpoint_path),
            torch_dtype=self.precision,
            is_trainable=False
        )
        model.eval()
        return model
    
    def infer_beam_search(
        self,
        images: List[Image.Image],
        texts: Optional[List[str]] = None,
        num_beams: int = 5,
        max_tokens: int = 512,
        checkpoint: str = "checkpoint-250",
    ) -> List[str]:
        """
        Beam search inference for improved predictions
        
        Args:
            images: List of PIL Image objects
            texts: Optional prompt texts (uses default if not provided)
            num_beams: Number of beams (higher = slower but better)
            max_tokens: Maximum tokens to generate
            checkpoint: Which checkpoint to use
            
        Returns:
            List of predicted Odia text
        """
        
        if texts is None:
            texts = ["Extract and read all Odia text from this image."] * len(images)
        
        model = self.load_checkpoint(checkpoint)
        predictions = []
        
        with torch.no_grad():
            inputs = self.processor(
                images=images,
                text=texts,
                return_tensors="pt",
                padding=True,
            ).to(self.base_model.device)
            
            outputs = model.generate(
                **inputs,
                num_beams=num_beams,
                early_stopping=True,
                max_new_tokens=max_tokens,
            )
            
            for output in outputs:
                pred_text = self.processor.decode(output, skip_special_tokens=True).strip()
                predictions.append(pred_text)
        
        torch.cuda.empty_cache()
        return predictions
    
    def infer_ensemble_voting(
        self,
        images: List[Image.Image],
        texts: Optional[List[str]] = None,
        checkpoints: Optional[List[str]] = None,
        num_beams: int = 3,
        max_tokens: int = 512,
        voting_method: str = "longest",
    ) -> List[str]:
        """
        Ensemble voting from multiple checkpoints
        
        Args:
            images: List of PIL Image objects
            texts: Optional prompt texts
            checkpoints: List of checkpoint names (all if not provided)
            num_beams: Beams per checkpoint
            max_tokens: Maximum tokens
            voting_method: 'longest', 'confidence', or 'majority'
            
        Returns:
            List of ensemble predicted text
        """
        
        if checkpoints is None:
            checkpoints = ["checkpoint-50", "checkpoint-100", "checkpoint-150", 
                          "checkpoint-200", "checkpoint-250"]
        
        if texts is None:
            texts = ["Extract and read all Odia text from this image."] * len(images)
        
        # Get predictions from each checkpoint
        all_predictions = []
        
        for ckpt in checkpoints:
            try:
                model = self.load_checkpoint(ckpt)
                
                with torch.no_grad():
                    inputs = self.processor(
                        images=images,
                        text=texts,
                        return_tensors="pt",
                        padding=True,
                    ).to(self.base_model.device)
                    
                    outputs = model.generate(
                        **inputs,
                        num_beams=num_beams,
                        max_new_tokens=max_tokens,
                    )
                    
                    for i, output in enumerate(outputs):
                        pred_text = self.processor.decode(output, skip_special_tokens=True).strip()
                        
                        if i >= len(all_predictions):
                            all_predictions.append([])
                        all_predictions[i].append(pred_text)
                
                torch.cuda.empty_cache()
                
            except Exception as e:
                print(f"‚ö†Ô∏è  Error loading {ckpt}: {e}")
                continue
        
        # Vote for best predictions
        final_predictions = []
        
        for preds in all_predictions:
            if not preds:
                final_predictions.append("")
            elif voting_method == "longest":
                # Choose longest (usually most complete)
                final_predictions.append(max(preds, key=len))
            elif voting_method == "majority":
                # Majority vote (most common prediction)
                from collections import Counter
                final_predictions.append(Counter(preds).most_common(1)[0][0])
            else:
                final_predictions.append(preds[-1])  # Just use last
        
        return final_predictions
    
    def infer_temperature_sampling(
        self,
        images: List[Image.Image],
        texts: Optional[List[str]] = None,
        temperature: float = 0.7,
        top_p: float = 0.9,
        max_tokens: int = 512,
        checkpoint: str = "checkpoint-250",
    ) -> List[str]:
        """
        Temperature-based sampling for better predictions
        
        Args:
            images: List of PIL Image objects
            texts: Optional prompt texts
            temperature: Lower = more confident (0.1-1.0)
            top_p: Nucleus sampling threshold
            max_tokens: Maximum tokens
            checkpoint: Which checkpoint to use
            
        Returns:
            List of predicted Odia text
        """
        
        if texts is None:
            texts = ["Extract and read all Odia text from this image."] * len(images)
        
        model = self.load_checkpoint(checkpoint)
        predictions = []
        
        with torch.no_grad():
            inputs = self.processor(
                images=images,
                text=texts,
                return_tensors="pt",
                padding=True,
            ).to(self.base_model.device)
            
            outputs = model.generate(
                **inputs,
                temperature=temperature,
                top_p=top_p,
                top_k=50,
                max_new_tokens=max_tokens,
                do_sample=True,
            )
            
            for output in outputs:
                pred_text = self.processor.decode(output, skip_special_tokens=True).strip()
                predictions.append(pred_text)
        
        torch.cuda.empty_cache()
        return predictions


# ============================================================================
# CONVENIENCE FUNCTIONS
# ============================================================================

def batch_inference_beam_search(
    image_paths: List[str],
    num_beams: int = 5,
    batch_size: int = 1,
) -> List[str]:
    """High-level function for batch beam search inference"""
    
    engine = OdiaOCRInferenceEngine()
    predictions = []
    
    for i in range(0, len(image_paths), batch_size):
        batch_paths = image_paths[i:i + batch_size]
        images = [Image.open(p).convert("RGB") for p in batch_paths]
        batch_preds = engine.infer_beam_search(images, num_beams=num_beams)
        predictions.extend(batch_preds)
    
    return predictions


def batch_inference_ensemble(
    image_paths: List[str],
    batch_size: int = 1,
) -> List[str]:
    """High-level function for batch ensemble inference"""
    
    engine = OdiaOCRInferenceEngine()
    predictions = []
    
    for i in range(0, len(image_paths), batch_size):
        batch_paths = image_paths[i:i + batch_size]
        images = [Image.open(p).convert("RGB") for p in batch_paths]
        batch_preds = engine.infer_ensemble_voting(images)
        predictions.extend(batch_preds)
    
    return predictions


# ============================================================================
# EXAMPLE USAGE
# ============================================================================

if __name__ == "__main__":
    print("\n" + "="*80)
    print("Odia OCR Inference Engine - Examples")
    print("="*80 + "\n")
    
    # Example 1: Beam Search
    print("Example 1: Beam Search Decoding")
    print("-" * 40)
    
    engine = OdiaOCRInferenceEngine()
    
    # Load a test image
    try:
        from datasets import load_dataset
        dataset = load_dataset("shantipriya/odia-ocr-merged", split="train")
        sample_img = dataset[0]["image"] if isinstance(dataset[0]["image"], Image.Image) else Image.open(dataset[0]["image"])
        
        # Beam search inference
        predictions = engine.infer_beam_search([sample_img], num_beams=5)
        print(f"‚úÖ Beam search prediction:\n   {predictions[0][:100]}...\n")
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not run example: {e}\n")
    
    # Example 2: Ensemble
    print("Example 2: Ensemble Voting")
    print("-" * 40)
    print("Usage: ensemble_preds = engine.infer_ensemble_voting([image1, image2], ...")
    print("       checkpoints=['checkpoint-250', 'checkpoint-200', ...])")
    print("       voting_method='longest'  # or 'majority'\n")
    
    # Example 3: Temperature Sampling
    print("Example 3: Temperature-based Sampling")
    print("-" * 40)
    print("Usage: temp_preds = engine.infer_temperature_sampling([image1, image2], ...")
    print("       temperature=0.7,  # Lower = more confident")
    print("       top_p=0.9)        # Nucleus sampling\n")
    
    print("="*80)
    print("‚úÖ Inference engine ready for production use!")
    print("="*80 + "\n")
